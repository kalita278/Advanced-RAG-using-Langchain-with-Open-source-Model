{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "#from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader('./data')\n",
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the doc\n",
    "split_doc = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mrinal Kalita\\Python Projects\\RAG using langchain\\myvenv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#loading Huggingface embedings\n",
    "emb = HuggingFaceBgeEmbeddings( model_name=\"BAAI/bge-small-en-v1.5\",model_kwargs={'device':'cpu'},encode_kwargs={'normalize_embeddings':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector stores\n",
    "db = FAISS.from_documents(split_doc,emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating retriever\n",
    "retriver = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 6\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "os.environ['huggingfacehub_api_token'] = os.getenv('HUGGINGFACE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mrinal Kalita\\Python Projects\\RAG using langchain\\myvenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what is machine learning?\\n\\nMachine learning is a subset of artificial intelligence that focuses on the development of computer programs that can learn from data without being explicitly programmed. Machine learning algorithms are used to build models that can make predictions or decisions based on data.\\n\\nMachine learning is a type of artificial intelligence that allows computers to learn from data without being explicitly programmed. Machine learning algorithms are used to build models that can make predictions or decisions based on data.\\n\\nMachine learning is a type of artificial intelligence that'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500})\n",
    "    #huggingfacehub_api_token=\"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\")\n",
    "\n",
    "llm.invoke(\"what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ('''\n",
    "    - You are an assistant for question-answering tasks. \n",
    "    - Use the following pieces of retrieved context to answer the question. \n",
    "    - If you don't know the answer, say that you don't know. \n",
    "    - Use three sentences maximum and keep the answer concise.\n",
    "    - dont use prompt messages in the answer\n",
    "    \\n\\n\n",
    "    {context}'''\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = create_stuff_documents_chain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_retrieval_chain(retriver,stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: \\n    - You are an assistant for question-answering tasks. \\n    - Use the following pieces of retrieved context to answer the question. \\n    - If you don't know the answer, say that you don't know. \\n    - Use three sentences maximum and keep the answer concise.\\n    - dont use prompt messages in the answer\\n    \\n\\n\\n    The most commonly used such extension to RNNs is the long short-term mem-\\nory(LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the con-long short-term\\nmemory\\ntext management problem into two subproblems: removing information no longer\\nneeded from the context, and adding information likely to be needed for later de-\\ncision making. The key to solving both problems is to learn how to manage this\\ncontext rather than hard-coding a strategy into the architecture. LSTMs accomplish\\nthis by ﬁrst adding an explicit context layer to the architecture (in addition to the\\nusual recurrent hidden layer), and through the use of specialized neural units that\\nmake use of gates to control the ﬂow of information into and out of the units that\\ncomprise the network layers. These gates are implemented through the use of addi-\\ntional weights that operate sequentially on the input, and previous hidden layer, and\\nprevious context layers.\\n\\n16 CHAPTER 9 • RNN S AND LSTM S\\n+\\nxtht-1cthtcthtct-1ht-1xttanh\\n+σtanhσσ+++igf\\no㽋㽋㽋LSTMct-1\\nFigure 9.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the\\ncurrent input, x, the previous hidden state, ht−1, and the previous context, ct−1. The outputs are a new hidden\\nstate, htand an updated context, ct.\\nh\\nxxtxtht-1htht\\nct-1ct\\nht-1(b)(a)(c)⌃gza⌃gzLSTMUnita\\nFigure 9.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and\\nlong short-term memory (LSTM).\\nAt the far left, (a) is the basic feedforward unit where a single set of weights and\\na single activation function determine its output, and when arranged in a layer there\\nare no connections among the units in the layer. Next, (b) represents the unit in a\\nsimple recurrent network. Now there are two inputs and an additional set of weights\\nto go with it. However, there is still a single activation function and output.\\n\\nht=ot⊙tanh(ct) (9.27)\\nFig. 9.13 illustrates the complete computation for a single LSTM unit. Given the\\nappropriate weights for the various gates, an LSTM accepts as input the context\\nlayer, and hidden layer from the previous time step, along with the current input\\nvector. It then generates updated context and hidden vectors as output.\\nIt is the hidden state, ht, that provides the output for the LSTM at each time step.\\nThis output can be used as the input to subsequent layers in a stacked RNN, or at the\\nﬁnal layer of a network htcan be used to provide the ﬁnal output of the LSTM.\\n9.5.1 Gated Units, Layers and Networks\\nThe neural units used in LSTMs are obviously much more complex than those used\\nin basic feedforward networks. Fortunately, this complexity is encapsulated within\\nthe basic processing units, allowing us to maintain modularity and to easily exper-\\niment with different architectures. To see this, consider Fig. 9.14 which illustrates\\n\\n2CHAPTER 9 • RNN S AND LSTM S\\nworks . These networks are useful in their own right and serve as the basis for more\\ncomplex approaches like the Long Short-Term Memory (LSTM) networks discussed\\nlater in this chapter. In this chapter when we use the term RNN we’ll be referring to\\nthese simpler more constrained networks (although you will often see the term RNN\\nto mean any net with recurrent properties including LSTMs).\\nxthtyt\\nFigure 9.1 Simple recurrent neural network after Elman (1990). The hidden layer includes\\na recurrent connection as part of its input. That is, the activation value of the hidden layer\\ndepends on the current input as well as the activation value of the hidden layer from the\\nprevious time step.\\nFig. 9.1 illustrates the structure of an RNN. As with ordinary feedforward net-\\nworks, an input vector representing the current input, xt, is multiplied by a weight\\nmatrix and then passed through a non-linear activation function to compute the val-\\nHuman: what is LSTM?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({'input':'what is LSTM'})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
